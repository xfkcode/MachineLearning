{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **sklearn学习笔记2**⭐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧰[***sklearn.feature_extraction.text.TfidfVectorizer***](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "```python\n",
    "class sklearn.feature_extraction.text.TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, \n",
    "lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), \n",
    "max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, \n",
    "norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "```\n",
    "- 作用：将原始文档集合转换为TF-IDF功能矩阵。  \n",
    "  (Convert a collection of raw documents to a matrix of TF-IDF features.)\n",
    "- Parameters：\n",
    "  - 🌡️ <b><i><font color=#FF0000 > input:{‘filename’, ‘file’, ‘content’}, default=’content’ </font></i></b> ✔️\n",
    "    - 如果为 `filename` ，则作为参数传递的序列应为需要读取以获取原始内容以进行分析的文件名列表。\n",
    "    - 如果为 `file` ，序列项必须具有“read”方法（类似文件的对象），该方法被调用以获取内存中的字节。\n",
    "    - 如果为 `content` ，则输入应为字符串或字节类型的项序列。\n",
    "  - ***encoding:str, default=’utf-8’***  \n",
    "    - 如果要分析字节或文件，则使用此编码进行解码。\n",
    "  - ***decode_error:{‘strict’, ‘ignore’, ‘replace’}, default=’strict’***  \n",
    "    - 关于如果要分析的字节序列包含不属于给定编码的字符，该如何说明。  \n",
    "    默认情况下，它是 `strict` ，这意味着将引发UnicodeDecodeError。其他值为“忽略”和“替换”。\n",
    "  - ***strip_accents:{‘ascii’, ‘unicode’}, default=None***  \n",
    "    - 在预处理步骤中删除重音并执行其他字符规范化，  \n",
    "      `ascii` 是一种快速方法，仅适用于具有直接ASCII映射的字符，  \n",
    "      `unicode` 是一种稍慢的方法，适用于任何字符。无（默认值）不执行任何操作。\n",
    "  - ***lowercase:bool, default=True***  \n",
    "    - 在标记化之前，将所有字符转换为小写。\n",
    "  - ***preprocessor:callable, default=None***  \n",
    "    - 覆盖 preprocessing（strip_accents 和 lowercase）阶段，同时保留 tokenizing 和 n-grams generation 步骤。  \n",
    "    仅当 analyzer 不可调用时才适用。\n",
    "  - ***tokenizer:callable, default=None***  \n",
    "    - 覆盖 string tokenization 步骤，同时保留 preprocessing 和 n-grams generation 步骤。  \n",
    "    仅当 analyzer=='word' 时才适用。\n",
    "  - ***stop_words:{‘english’}, list, default=None***  \n",
    "    - 如果是 `english` ，则使用内置的英语停止词列表。  \n",
    "    - 如果是 列表，则假定该列表包含停止词，所有这些都将从生成的标记中删除。  \n",
    "      仅当 analyzer=='word' 时才适用。\n",
    "    - 如果 `None` ，则不会使用停止词。\n",
    "  - ***token_pattern:str, default=r”(?u)\\b\\w\\w+\\b”***  \n",
    "    - 正则表达式，表示什么构成 'token'，仅在 analyzer=='word' 时使用。  \n",
    "      默认选择2个或更多字母数字字符的标记（标点符号被完全忽略，并且始终被视为标记分隔符）。\n",
    "  - 🌡️ <b><i><font color=#FF0000 > ngram_range:tuple (min_n, max_n), default=(1, 1) </font></i></b> ✔️\n",
    "    - 要提取的不同单词n-gram或字符n-gram的n值范围的下限和上限。  \n",
    "      ngram_range为（1，1）表示仅单字，（1，2）表示单字和双字，（2，2）仅表示双字。  \n",
    "      仅当 analyzer 不可调用时才适用。\n",
    "  - ***analyzer:{‘word’, ‘char’, ‘char_wb’} or callable, default=’word’***  \n",
    "    - 特征应该由单词n-gram还是字符n-gram组成。  \n",
    "      选项 `char_wb` 仅从单词边界内的文本创建字符n-gram；单词边缘的n-gram用空格填充。\n",
    "  - ***max_df:float in range [0.0, 1.0] or int, default=1.0***  \n",
    "    - 构建词汇表时，忽略文档频率严格高于给定阈值的术语（语料库特定的停止词）。  \n",
    "      如果为float，则参数表示文档的比例，整数绝对计数。如果词汇表不是None，则忽略此参数。\n",
    "  - ***min_df:float in range [0.0, 1.0] or int, default=1***  \n",
    "    - 构建词汇表时，忽略文档频率严格低于给定阈值的术语。该值在文献中也称为截止值。    \n",
    "      如果为float，则参数表示文档的比例，整数绝对计数。如果词汇表不是None，则忽略此参数。\n",
    "  - ***max_features:int, default=None***  \n",
    "    - 如果不是 `None` ，则构建一个词汇表，该词汇表只考虑语料库中按词频排序的最大特征。  \n",
    "      如果词汇表不是None，则忽略此参数。\n",
    "  - ***vocabulary:Mapping or iterable, default=None***  \n",
    "    - 映射（例如，dict），其中键是项，值是特征矩阵中的索引，或者是可迭代项。  \n",
    "      如果没有给出，则根据输入文档确定词汇表。  \n",
    "      映射中的索引不应重复，并且在0和最大索引之间不应有任何间隙。\n",
    "  - 🌡️ <b><i><font color=#FF0000 > binary:bool, default=False </font></i></b> ✔️\n",
    "    - 如果为 `True` ，则所有非零计数都设置为1。  \n",
    "      这对于建模二进制事件而非整数计数的离散概率模型非常有用。\n",
    "  - ***dtype:type, default=np.int64***  \n",
    "    - fit_transform()或transformm()返回的矩阵类型。 \n",
    "  - 🔥 <b><i><font color=green >norm:{‘l1’, ‘l2’}, default=’l2’</font></i></b> ⭕\n",
    "    - 每个输出行将具有单位范数，或者：\n",
    "      - `l2`：矢量元素的平方和为1。当应用l2范数时，两个矢量之间的余弦相似度是它们的点积。\n",
    "      - `l1`：矢量元素的绝对值之和为1。\n",
    "  - 🔥 <b><i><font color=green >use_idf:bool, default=True</font></i></b> ⭕\n",
    "    - 启用反向文档频率重新加权。如果为False，则 $idf(t)=1$。\n",
    "  - 🔥 <b><i><font color=green >smooth_idf:bool, default=True</font></i></b> ⭕\n",
    "    - 通过向文档频率添加一个来平滑idf权重，就像看到一个额外的文档恰好包含了集合中的每个术语一样。  \n",
    "      防止零分割。\n",
    "  - 🔥 <b><i><font color=green >sublinear_tf:bool, default=False</font></i></b> ⭕\n",
    "    - 应用次线性tf缩放，即将tf替换为 $1+log(tf)$。\n",
    "\n",
    "| **Methods**                                                  | **Method Description**                       |\n",
    "| :----------------------------------------------------------- | :------------------------------------------- |\n",
    "| [`build_analyzer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.build_analyzer)() | 返回处理输入数据的可调用函数。               |\n",
    "| [`build_preprocessor`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.build_preprocessor)() | 返回一个函数，在标记化之前对文本进行预处理。 |\n",
    "| [`build_tokenizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.build_tokenizer)() | 返回将字符串拆分为标记序列的函数。           |\n",
    "| [`decode`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.decode)(doc) | 将输入解码为一串unicode符号。                |\n",
    "| [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit)(raw_documents[, y]) | 学习原始文档中所有标记的词汇词典。           |\n",
    "| [`fit_transform`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform)(raw_documents[, y]) | 学习词汇词典并返回文档术语矩阵。             |\n",
    "| [`get_feature_names_out`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names_out)([input_features]) | 获取用于转换的输出功能名称。                 |\n",
    "| [`get_params`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.get_params)([deep]) | 获取此估计器的参数。                         |\n",
    "| [`get_stop_words`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.get_stop_words)() | 建立或获取有效的停止词列表。                 |\n",
    "| [`inverse_transform`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.inverse_transform)(X) | 返回X中非零条目的每个文档的术语。            |\n",
    "| [`set_params`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.set_params)(**params) | 设置此估计器的参数。                         |\n",
    "| [`transform`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.transform)(raw_documents) | 将文档转换为文档术语矩阵。                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?', ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a825f8e8905b8e64233e6384cfa1616d0baa442a3b458fa78e4916d5c671b0d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
