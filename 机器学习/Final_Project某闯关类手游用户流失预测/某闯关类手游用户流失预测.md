# Final Projectï¼šæŸé—¯å…³ç±»æ‰‹æ¸¸ç”¨æˆ·æµå¤±é¢„æµ‹â­

**æŸé—¯å…³ç±»æ‰‹æ¸¸ç”¨æˆ·æµå¤±é¢„æµ‹**   
***user_id*** ç”¨æˆ·ID ğŸ‘½ ***num_attempts*** å°è¯•æ¬¡æ•° â³ ***clear_rate*** é€šå…³ç‡ ğŸ±   
***avg_duration*** å¹³å‡ç”¨æ—¶ â±ï¸ ***avg_reststep*** å¹³å‡å‰©ä½™æ­¥æ•°æ¯” ğŸ‘£   
***cum_help*** ç´¯ç§¯å¸®åŠ©æ¬¡æ•° âœ‹ ***landing_days*** ç™»é™†å¤©æ•° ğŸ“… ***label*** æµå¤± âœ”ï¸âŒ

## **ğŸ¥½**æ•°æ®æ¦‚è§ˆ

æœ¬æ¬¡æ•°æ®å’Œä»¥å¾€ç»“æ„åŒ–çš„å½¢å¼ä¸åŒï¼Œå±•ç°çš„æ˜¯æ›´åŸå§‹çš„æ•°æ®è®°å½•ï¼Œæ›´æ¥è¿‘å…¬å¸å®é™…æ—¥å¿—çš„å½¢å¼  
å…±åŒ…å« ***5*** ä¸ªæ–‡ä»¶ï¼š  
ğŸ—ƒï¸ **level_seq.csv** ğŸ“® **level_meta.csv**   
ğŸšŠ **train.csv** â­• **dev.csv** ğŸ§ª **test.csv**  

#### ğŸ”°å¯¼å…¥æ•°æ®
- **è®­ç»ƒé›†**ï¼š"./data/" ğŸ“‚è·¯å¾„ä¸‹ *train.csv* ğŸ“æ–‡ä»¶
  åŒ…å«æ€»å…± **8158** æ¡æ•°æ®æ ·æœ¬ ğŸ’¾
  æ¯æ¡æ ·æœ¬åŒ…å« **user_id/label**
- **éªŒè¯é›†**ï¼š"./data/" ğŸ“‚è·¯å¾„ä¸‹ *dev.csv* ğŸ“æ–‡ä»¶
  åŒ…å«æ€»å…± **2658** æ¡æ•°æ®æ ·æœ¬ ğŸ’¾
  æ¯æ¡æ ·æœ¬åŒ…å« **user_id/label** 
  [ğŸ”§]ï¼šå¯è¿›è¡Œè¶…å‚æ•°çš„çš„è°ƒæ•´ï¼›ä¹Ÿå¯ä½œä¸ºç¦»çº¿æµ‹è¯•é›†ï¼Œè¿›è¡Œæµ‹è¯•
- **æµ‹è¯•é›†**ï¼š"./data/" ğŸ“‚è·¯å¾„ä¸‹ *test.csv* ğŸ“æ–‡ä»¶
  åŒ…å«æ€»å…± **2773** æ¡æ•°æ®æ ·æœ¬ ğŸ’¾
  æ¯æ¡æ ·æœ¬ä»…åŒ…å« **user_id** 
  [â˜ï¸]ï¼šæµ‹è¯•é›†çœŸå®æ ‡ç­¾åœ¨äº‘ç«¯ï¼Œå¯ä¸Šä¼ ç»“æœè¯„ä¼°æ€§èƒ½

[***dataDownload*** **é“¾æ¥**]()ğŸ‘ˆ
[ğŸ“¢]ï¼šæ‰€æœ‰æ–‡ä»¶ *user_id* ç»Ÿä¸€ã€‚
âœ”ï¸âš™ï¸å€ŸåŠ© **pandas** è¯»å…¥æ ‡å‡† *csv* æ ¼å¼æ–‡ä»¶çš„å‡½æ•° `read_csv()` å°†æ•°æ®è½¬æ¢ä¸º `DataFrame` çš„å½¢å¼ã€‚

```python
# è¯»å…¥csvæ–‡ä»¶ä¸ºpandasçš„DataFrame
seq_df = pd.read_csv('./data/level_seq.csv', sep='\t')
meta_df = pd.read_csv('./data/level_meta.csv', sep='\t')
train_df = pd.read_csv('./data/train.csv', sep='\t')
dev_df = pd.read_csv('./data/dev.csv', sep='\t')
test_df = pd.read_csv('./data/test.csv', sep='\t')
```

## **ğŸ§ **ç‰¹å¾å·¥ç¨‹

æ ¹æ® **level_seq.csv** é’ˆå¯¹ **ç”¨æˆ·** æå–ç‰¹å¾

#### ğŸ—ƒï¸level_seq.csv
ğŸ’šæ ¸å¿ƒæ•°æ®æ–‡ä»¶ğŸ’š 
åŒ…å«ç”¨æˆ·æ¸¸ç©æ¯ä¸ªå…³å¡çš„è®°å½•ï¼Œæ¯ä¸€æ¡è®°å½•æ˜¯å¯¹æŸä¸ªå…³å¡çš„ä¸€æ¬¡å°è¯•ï¼Œå…·ä½“æ¯åˆ—çš„å«ä¹‰å¦‚ä¸‹ï¼š

- `user_id`ï¼šç”¨æˆ· idï¼Œå’Œè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†ä¸­çš„å¯ä»¥åŒ¹é…ï¼›

* `level_id`ï¼šå…³å¡ id
* `f_success`ï¼šæ˜¯å¦é€šå…³ï¼ˆ1ï¼šé€šå…³ï¼Œ0ï¼šå¤±è´¥ï¼‰
* `f_duration`ï¼šæ­¤æ¬¡å°è¯•æ‰€ç”¨çš„æ—¶é—´ï¼ˆå•ä½ sï¼‰
* `f_reststep`ï¼šå‰©ä½™æ­¥æ•°ä¸é™å®šæ­¥æ•°ä¹‹æ¯”ï¼ˆå¤±è´¥ä¸º 0ï¼‰
* `f_help`ï¼šæ˜¯å¦ä½¿ç”¨äº†é“å…·ã€æç¤ºç­‰é¢å¤–å¸®åŠ©ï¼ˆ1ï¼šä½¿ç”¨ï¼Œ0ï¼šæœªä½¿ç”¨ï¼‰
* `time`ï¼šæ—¶é—´æˆ³

#### â™Ÿï¸ç‰¹å¾ Features

|      | user_id | å°è¯•æ¸¸æˆæ¬¡æ•° |   é€šå…³ç‡ | æ¸¸æˆå¹³å‡ç”¨æ—¶ | å¹³å‡å‰©ä½™æ­¥æ•°æ¯” | ç´¯ç§¯å¸®åŠ©æ¬¡æ•° | ç™»é™†å¤©æ•° | label |
| ---: | ------: | -----------: | -------: | -----------: | -------------: | -----------: | -------: | ----: |
|    0 |    2774 |          215 | 0.632558 |        118.1 |       0.189056 |           18 |        4 |     0 |
|    1 |    2775 |          111 | 0.738739 |        169.7 |       0.258456 |           14 |        3 |     0 |
|    2 |    2776 |           69 | 0.637681 |         88.7 |       0.186543 |            1 |        3 |     1 |
|    3 |    2777 |          286 | 0.506993 |        142.7 |       0.124245 |            4 |        4 |     0 |
|    4 |    2778 |          162 | 0.672840 |        197.8 |       0.299450 |            9 |        3 |     1 |
|  ... |     ... |          ... |      ... |          ... |            ... |          ... |      ... |   ... |

```python
''' 
å‡½æ•°è¯´æ˜ï¼šæ ¹æ® t_df æ„å»ºç‰¹å¾
Parameters:
    df - level_seq.csv ç”¨æˆ·äº¤äº’æ•°æ®
    t_df - train.csv/dev.csv/test.csv è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†
Returns:
    features_df - ç‰¹å¾æ•°æ®é›†(DataFrame)
'''
def Features_Construct(df,t_df):
    features = []

    for i,user in enumerate(t_df['user_id']):
        user_features = []
        user_id = user
        user_features.append(user_id)

        user_df = seq_df.query('user_id=={}'.format(user_id))
        # ç”¨æˆ·å°è¯•æ¸¸æˆæ¬¡æ•°
        user_features.append(user_df.shape[0])
        user_df_succ= user_df.query('f_success==1')
        # é€šå…³ç‡
        success_rate = round(user_df_succ.shape[0]/user_df.shape[0],6)
        user_features.append(success_rate)
        # é€šè¿‡æœ€é«˜å…³å¡
        # num_max = np.array(user_df_succ['level_id']).max()
        # user_features.append(num_max)
        # æ¸¸æˆå¹³å‡ç”¨æ—¶
        duration_mean = round(np.array(user_df['f_duration']).mean(),1)
        user_features.append(duration_mean)
        # å¹³å‡reststep
        reststep_mean = round(np.array(user_df['f_reststep']).mean(),6)
        user_features.append(reststep_mean)
        # ç´¯ç§¯å¸®åŠ©æ¬¡æ•°
        times_help = np.array(user_df['f_help']).sum()
        user_features.append(times_help)
        # ç™»é™†å¤©æ•°
        time = np.array(user_df['time'])
        day = [i[9] for i in time]
        dd = Counter(day)
        days = len(dd)
        user_features.append(days)
        features.append(user_features)

    features_df = pd.DataFrame(features)
    features_df.columns =['user_id','å°è¯•æ¸¸æˆæ¬¡æ•°','é€šå…³ç‡','æ¸¸æˆå¹³å‡ç”¨æ—¶',
                          'å¹³å‡å‰©ä½™æ­¥æ•°æ¯”','ç´¯ç§¯å¸®åŠ©æ¬¡æ•°','ç™»é™†å¤©æ•°']
    return features_df
```

## **ğŸ“¡**æ•°æ®æ„å»º

- âš ï¸**å½’ä¸€åŒ–ï¼šMin-Max Normalization** 

$$
\frac{x_{i}-min(x_i)}{max(x_i)-min(x_i)}
$$

- ***DataFrame*** è½¬æ¢â†’ ***Array*** 
  `np.array()` / `df.values` 

```python
train_features.shape,train_labels.shape,dev_features.shape,dev_labels.shape
>> (8158, 6), (8158,), (2658, 6), (2658,)
test_features.shape
>> (2773, 6)
```

## **ğŸ§°**æ¨¡å‹æ„å»º

#### 1.å†³ç­–æ ‘

***sklearn.tree.DecisionTreeClassifier*** ğŸŒµ

```python
class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, smin_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)
```

#### 2.è´å¶æ–¯

***sklearn.naive_bayes.BernoulliNB/ MultinomialNB/ ComplementNB*** ğŸ§

```python
class sklearn.naive_bayes.BernoulliNB(*, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)
class sklearn.naive_bayes.MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)
class sklearn.naive_bayes.ComplementNB(*, alpha=1.0, fit_prior=True, class_prior=None, norm=False)
```

#### 3.K-è¿‘é‚»

***sklearn.neighbors.KNeighborsClassifier*** ğŸ›µ

```python
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', 
leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
```

#### 4.æ”¯æŒå‘é‡æœº

***sklearn.svm.SVC*** ğŸ›’

```python
class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)
```

#### ğŸ²Tuning

***sklearn.model_selection.GridSearchCV*** ğŸ”

```python
class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)
```

#### 5.é›†æˆå­¦ä¹ 

åŸºåˆ†ç±»å™¨âš¾ 
å‚æ•°åˆ†åˆ«æ ¹æ® **1-4** æœ€ä½³ **Accuracy** é€‰æ‹©
***GridSearchCV*** 

- ğŸŒµ ***DT***ï¼š  `DecisionTreeClassifier(criterion='entropy', max_depth=2, min_samples_split=50)`
- ğŸ§***NB***ï¼š  `BernoulliNB()`
- ğŸ›µ***KNN***ï¼š `KNeighborsClassifier(metric='chebyshev', n_neighbors=29, weights='uniform')`
- ğŸ›’***SVM***ï¼š `SVC(C=1, kernel='rbf', gamma=0.01)`

***sklearn.ensemble.BaggingClassifier*** ğŸ›ï¸

```python
class sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)
```

***sklearn.ensemble.AdaBoostClassifier*** ğŸ¥¾

```python
class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, *, n_estimators=50, 
learning_rate=1.0, algorithm='SAMME.R', random_state=None)
```

## ğŸ’¯æ¨¡å‹è¯„ä¼° AUC(dev) 

#### ğŸŒµDT/ ğŸ§NB/ ğŸ›µKNN/ ğŸ›’SVM (ROC)

![image-20221122094602715](G:\Githubä»“åº“\MachineLearning\æœºå™¨å­¦ä¹ \Final_ProjectæŸé—¯å…³ç±»æ‰‹æ¸¸ç”¨æˆ·æµå¤±é¢„æµ‹\æŸé—¯å…³ç±»æ‰‹æ¸¸ç”¨æˆ·æµå¤±é¢„æµ‹.assets\image-20221122094602715.png) 

```python
D_T model: 
>> AUC = 0.7663
B_N model: 
>> AUC = 0.7047
KNN model: 
>> AUC = 0.7624
SVM model: 
>> AUC = 0.7730
```

#### ğŸ¥¡Ensemble learning+ AUC å¯¹æ¯”

```python
Bagging + D_T model: 
>> AUC = 0.7703
Bagging + B_N model: 
>> AUC = 0.7045
Bagging + KNN model: 
>> AUC = 0.7606
Bagging + SVM model: 
>> AUC = 0.7764
```

```python
Adaboost + D_T model: 
>> AUC = 0.7826
Adaboost + B_N model: 
>> AUC = 0.7045
Adaboost + SVM model: 
>> AUC = 0.5000
```

#### ğŸ”¬ Result åˆ†æ

|                | **D_T**                                          | **N_B** | **KNN** | **SVM**                                      |
| :------------- | ------------------------------------------------ | ------- | ------- | :------------------------------------------- |
| **Baseline**   | 0.7663                                           | 0.7047  | 0.7624  | **<font color=CornflowerBlue>0.7730</font>** |
| **Bagging +**  | 0.7703 (up 0.5%)                                 | 0.7045  | 0.7606  | 0.7764 (up 0.4%)                             |
| **Adaboost +** | **<font color=DeepPink>0.7826</font>** (up 2.1%) | 0.7045  | NaN     | 0.5000                                       |

- å¯¹æ¯” **Baseline** Best model:
  - **SVM(dev) = 0.7730** 

- 1ï¸âƒ£Bagging + D_T ç›¸æ¯” D_T æå‡ 0.5%ï¼›2ï¸âƒ£Adaboost + D_T ç›¸æ¯” D_T æå‡ 2.1%ï¼›3ï¸âƒ£Bagging + SVM ç›¸æ¯” SVM æå‡ 0.4%
  N_B æ€§èƒ½å¹³å¹³ï¼Œé›†æˆå­¦ä¹ + å¹¶æ— æ˜æ˜¾æ”¹å–„
  AdaBoost + SVM æ€§èƒ½éª¤é™
  [ğŸ“¢]ï¼šKNN æ²¡æœ‰ `fit(X, y, sample_weight=None)` æ— æ³•é€‚é… Adaboost
- **Best model** ğŸ¥‡
  **\>\> AUC(dev) = 0.7826**  
  - [x] **Adaboost + D_T model** ğŸ‘ˆ

## â˜ï¸æµ‹è¯•é›† result.csv 

|          0 |        1 |        2 |        3 |        4 |       5 |        6 |       7 | **...** |
| ---------: | -------: | -------: | -------: | -------: | ------: | -------: | ------: | ------: |
|         ID |        1 |        2 |        3 |        4 |       5 |        6 |       7 | **...** |
| Prediction | 0.292253 | 0.348539 | 0.487569 | 0.359274 | 0.32392 | 0.353699 | 0.49843 | **...** |

[â˜ï¸]ï¼š**æ‰“æ¦œæˆç»©**â«  

- **Adaboost + D_T model** 
  **\>\> AUC(test) = 0.77304** â—â—â—

\---

\> âœï¸ [é‚¢ç¦å‡¯ (xfkcode@github)](https://github.com/xfkcode)  

\> ğŸ“… **å†™äº 2022å¹´11æœˆ**
