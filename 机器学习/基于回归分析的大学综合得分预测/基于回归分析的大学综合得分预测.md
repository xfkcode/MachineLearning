# 基于回归分析的大学综合得分预测⭐
🏫quality_of_education（教育质量）  
👨‍🎓alumni_employment（校友就业）  
👨‍🏫quality_of_faculty（教师质量）  
📚publications（出版物）  
📈influence（影响力）  
✒️citations（引用）  
🌐broad_impact（广泛影响）  
📝patents（专利）  
## 🧩多元线性回归
- $Y=X\beta+\epsilon$

$$
Y=\begin {bmatrix}
   y_1\\
   y_2\\
   {\vdots}\\
   y_n
  \end{bmatrix},
X=\begin {bmatrix}
        1 &x_{12} &{\cdots} &x_{1k}\\
        1 &x_{22} &{\cdots} &x_{2k}\\
        {\vdots}&{\vdots}&{\ddots}&{\vdots}\\
        1 &x_{n2} &{\cdots} &x_{nk}
       \end{bmatrix},
\beta=\begin {bmatrix}
        \beta_1\\
        \beta_2\\
        {\vdots}\\
        \beta_n
       \end{bmatrix},
\epsilon=\begin {bmatrix}
        \epsilon_1\\
        \epsilon_2\\
        {\vdots}\\
        \epsilon_n
       \end{bmatrix}
$$
 - 误差项:  

$$
e=\begin {bmatrix}
  e_1\\
  e_2\\
  {\vdots}\\
  e_n\\
  \end{bmatrix}=Y-X\beta
$$
 - 损失函数：
    $\displaystyle \sum_{i=1}^n{e_i^2}=e^Te$
    
 - 求解: $\frac{\partial e^Te}{\partial \beta}=-2X^TY+2X^TX\beta=0$  

 - 📌 $\beta=(X^TX)^{-1}X^TY$ 📌
## 🧩局部加权线性回归(Locally Weighted Linear Regression)
- 📌 $\beta=(X^TWX)^{-1}X^TWY$ 📌
  
  - $w(i,i)=exp(\frac{|x^{(i)}-x|^2}{-2k^2})$  
  
    LWLR使用"核"（与支持向量机中的核类似）来对附近的点赋予更高的权重。  
    核的类型可以自由选择，最常用的核就是高斯核。
## 🧩Lasso回归
- Lasso回归即我们所说的L1正则线性回归，在一般的线性回归最小化均方误差的基础上增加了一个参数 $\beta$ 的L1范数的罚项，  
从而最小化罚项残差平方和： 

  - $min||X\beta-Y||_2^2+\lambda||\beta||_1$ 
## 🧩岭回归(Ridge Regression)
- 📌 $\beta=(X^TWX+\lambda I)^{-1}X^TWY$ 📌  

- 岭回归即我们所说的L2正则线性回归，在一般的线性回归最小化均方误差的基础上增加了一个参数 $\beta$ 的L2范数的罚项，  
从而最小化罚项残差平方和： 

  - $min||X\beta-Y||_2^2+\lambda||\beta||_2^2$  

## 🧩前向逐步线性回归(Forward Stepwise Linear Regression)
- 🧠核心思想：根据最小平方误差迭代寻找最佳回归
- 算法：  
  初始化回归系数、损失值L  
  迭代循环：  
  &emsp;&emsp;属性循环：  
  &emsp;&emsp;&emsp;&emsp;增减循环：  
  &emsp;&emsp;&emsp;&emsp;对属性进行更新增减步长  
  &emsp;&emsp;&emsp;&emsp;计算当前预测值  
  &emsp;&emsp;&emsp;&emsp;计算平方误差和E  
  &emsp;&emsp;&emsp;&emsp;如果E < L：  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;保持修改  
  &emsp;&emsp;&emsp;&emsp;否则：  
  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;保持不变
## 💯误差分析RMSE
- [x] sklearn.linear_model.LinearRegression
  ```python
  from sklearn.linear_model import LinearRegression
  linear = LinearRegression()
  linear.fit(x_train, y_train)
  yp=linear.predict(x_test)
  RMSE: 3.563616720553335
- [x] 多元线性回归正规方程实现
  ```python
  RMSE: 15.619369247273445
- [x] 局部加权线性回归LWLR实现
  ```python
  RMSE: 8.487375720323163
  注：LWLR方法在多元线性回归正规方程的基础上将测试集样本点作为加权基准训练模型，预测误差有所提高。
- [x] sklearn.linear_model.Lasso
  ```python
  from sklearn.linear_model import Lasso
  lasso=Lasso(alpha=0.5)
  lasso.fit(x_train,y_train)
  yp=lasso.predict(x_test)
  RMSE: 3.562765782178886
- [x] sklearn.linear_model.Ridge
  ```python
  from sklearn.linear_model import Ridge
  ridge=Ridge(alpha=0.5)
  ridge.fit(x_train,y_train)
  yp=ridge.predict(x_test)
  RMSE: 3.5636166953496056
- [x] 岭回归RR正规方程实现
  ```python
  RMSE: 15.619369249715453
- [x] 前向逐步线性回归WSLR实现
  ```python
  RMSE: 15.624929088879103
  注：初始化回归系数全0得到的结果
  尝试：用FSLR方法对其他方法得到的结果进一步微调优化结果不明显。
## ⭕Try Something
- [x] 尝试用FSLR方法对多元线性正规方程实现和岭回归正规方程实现得到的结果进一步微调优化结果：  
- 即初始化用上述两个方法得到的结果，效果不佳，几乎没有变化😅
- ❗❗❗可忽略❗❗❗
---
> ✍️ [邢福凯 (xfkcode@github)](https://github.com/xfkcode)  
> 📅 *写于 2022年9月*