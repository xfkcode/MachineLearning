# 基于K-近邻的车牌号识别⭐
## K-近邻 ***(k-nearest neighbor, k-NN)***
- 原理：  
  一个样本数据集合，也称训练样本集，样本集中每个数据都存在标签，即样本集中每一个数据与所属分类的对应关系。  
  输入新数据，比较新数据与样本集数据对应的特征，然后算法提取样本集最相似数据(最近邻)的分类标签。  
  一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。  
  最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。
- 🧠***k*** **-** **近邻**算法 ***algorithm***
  1. 计算已知类别数据集中的点与当前点之间的距离；
  2. 按照距离递增次序排序；
  3. 选取与当前点距离最小的 ***k*** 个点；
  4. 确定前 ***k*** 个点所在类别的出现频率；
  5. 返回前 ***k*** 个点所出现频率最高的类别作为当前点的预测分类。
- 📐距离度量  
  - ***Minkowski*** ( $L_{\lambda}$ )  
  $$📌d(i,j)=[\sum_{k=1}^{p} {(x_{ik}-x_{jk})^2}]^\frac{1}{\lambda}📌$$  
    - **欧几里得距离** ( $\lambda=2$ )  
  $$📌d_{ij}=\sqrt{\sum_{k=1}^{p} {(x_{ik}-x_{jk})^2}}📌$$ 
    - **曼哈顿距离** ( $\lambda=1$ )  
  $$📌d(i,j)=\sum_{k=1}^{p} {|(x_{k}(i)-x_{k}(j))|}📌$$
    - **切比雪夫距离** ( $L_{\infty}$ )
  $$📌d(i,j)=\underset{k}{max} {|(x_{k}(i)-x_{k}(j))|}📌$$ 
  - ***Bray-Curtis Dist***
  - ***Canberra Dist***
- 💾属性归一化 ***(Normalization)***  
  - ***Min-Max Normalization***
  $\frac{x_{i}-min(x_i)}{max(x_i)-min(x_i)}$  
  将训练集中某一列数值特征（假设是第 ***i*** 列）的值缩放到 **[** ***0,1*** **]** 之间。
  - ***Z—score*** **标准化**
  $\frac{x_{i}-\bar{x}}{\sigma(x)}$   
  将训练集中某一列数值特征（假设是第 ***i*** 列）的值缩放成 **均值** ***0***，**方差** ***1*** 的状态。

  📢***Log***,***Sum***,**指数**,**正切**... ...




---
> ✍️ [邢福凯 (xfkcode@github)](https://github.com/xfkcode)  
> 📅 *写于 2022年10月*