# åŸºäºé›†æˆå­¦ä¹ çš„ Amazon ç”¨æˆ·è¯„è®ºè´¨é‡é¢„æµ‹â­
***Amazon*** **ç”¨æˆ·è¯„è®ºè´¨é‡**  
***reviewerID*** ç”¨æˆ·ID ğŸ‘½
***asin*** å•†å“ID ğŸ‘”
***reviewText*** è¯„è®º ğŸ’¬
***overall*** å•†å“è¯„åˆ† ğŸ’¯  
***votes_up*** è¯„è®ºç‚¹èµæ•° ğŸ’š
***votes_all*** è¯„è®ºæ€»è¯„ä»·æ•° ğŸ—³ï¸
***label*** è¯„è®ºè´¨é‡ âœ”ï¸âŒ

## ğŸ“®**é›†æˆå­¦ä¹ ** ***(Ensemble Learning)***  
***'Three heads are better than one.'*** ğŸ¤”
- åŸºæœ¬æƒ³æ³•ğŸ’¡
  - æœ‰æ—¶ä¸€ä¸ªå•ä¸ªåˆ†ç±»å™¨è¡¨ç°ä¸å¥½ï¼Œä½†æ˜¯èåˆè¡¨ç°ä¸é”™
  - ç®—æ³•æ± ä¸­çš„æ¯ä¸€ä¸ªå­¦ä¹ å™¨éƒ½æœ‰å®ƒçš„æƒé‡
  - å½“éœ€è¦å¯¹ä¸€ä¸ªæ–°çš„å®ä¾‹ä½œé¢„æµ‹æ—¶
    - æ¯ä¸ªå­¦ä¹ å™¨ä½œå‡ºè‡ªå·±çš„é¢„æµ‹
    - ç„¶åä¸»ç®—æ³•æŠŠè¿™äº›ç»“æœæ ¹æ®æƒå€¼åˆå¹¶èµ·æ¥ï¼Œä½œå‡ºæœ€ç»ˆé¢„æµ‹
- é›†æˆç­–ç•¥ğŸ²
  - ğŸ°å¹³å‡
    - ç®€å•å¹³å‡
    - åŠ æƒå¹³å‡
  - ğŸ—³ï¸æŠ•ç¥¨
    - å¤šæ•°æŠ•ç¥¨æ³•
    - åŠ æƒæŠ•ç¥¨æ³•
  - ğŸ“—å­¦ä¹ 
    - åŠ æƒå¤šæ•°
    - å †å  ***(Stacking)***  
      ğŸ§±å±‚æ¬¡èåˆï¼ŒåŸºå­¦ä¹ å™¨çš„è¾“å‡ºä½œä¸ºæ¬¡å­¦ä¹ å™¨çš„è¾“å…¥

## âš–ï¸**åŠ æƒå¤šæ•°** ***(Weighted Majority)***
### ***Weighted Majority Algorithm***ğŸ§ 
$a_i$ æ˜¯ç®—æ³•æ± ä¸­ç¬¬ $i$ ä¸ªé¢„æµ‹ç®—æ³•ï¼Œæ¯ä¸ªç®—æ³•å¯¹è¾“å…¥ $X$ æœ‰äºŒå€¼è¾“å‡º $\lbrace 0,1\rbrace$  
$w_i$ å¯¹åº” $a_i$ çš„æƒå€¼
- $\forall i,w_i \leftarrow 1$
- å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬ $[x,c(x)]$
  - åˆå§‹åŒ– $q_0 \leftarrow q_1 \leftarrow 0$
  - å¯¹æ¯ä¸ªç®—æ³• $a_i$
    - **if** $a_i(x)=0$ :  
      &emsp; $q_0 \leftarrow q_0 + w_i$  
      **else** :  
      &emsp; $q_1 \leftarrow q_1 + w_i$ 
  - å¦‚æœ $q_0>q_1$ ,åˆ™é¢„æµ‹ $c(x)=0$ ,å¦åˆ™é¢„æµ‹ $c(x)=1$  
    ( $q_0=q_1$ æ—¶å–ä»»æ„å€¼ )
  - å¯¹æ¯ä¸ª $a_i\in A$
    - å¦‚æœ $a_i(x)=c(x)$ ,é‚£ä¹ˆ $w_i \leftarrow \beta w_i$  
      ( $\beta \in [0,1)$æƒ©ç½šç³»æ•° )  
      $\beta=0$ æ—¶æ˜¯ä½œç”¨åœ¨ $A$ ä¸Šçš„ <i><b><font color=Gold>Halving Algorithm</font></b></i>

## ğŸ¥¡***Bagging***
- **Bagging** = **B**ootstrap **agg**rega**ting**
- Bootstrap asmpling (æ‹”é´æ³•/è‡ªä¸¾æ³•é‡‡æ ·)
  - ç»™å®šé›†åˆ $D$ ,å«æœ‰ $m$ è®­ç»ƒæ ·æœ¬
  - é€šè¿‡ä» $D$ ä¸­å‡åŒ€éšæœºçš„æœ‰æ”¾å›é‡‡æ · $m$ ä¸ªæ ·æœ¬æ„å»º $D_i$
### ***Bagging Algorithm***ğŸ§ 
â™»ï¸**For** $t=1,2,\ldots,T$ **Do**
1. ä» $S$ ä¸­æ‹”é´é‡‡æ ·äº§ç”Ÿ $D_t$
2. åœ¨ $D_t$ ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ $H_t$

ğŸ¥½åˆ†ç±»ä¸€ä¸ªæ–°çš„æ ·æœ¬ $x\in X$ æ—¶ï¼Œé€šè¿‡å¯¹ $H_t$ å¤šæ•°æŠ•ç¥¨ [ğŸ—³ï¸]ï¼ˆç­‰æƒé‡ï¼‰
```python
class Bagging():
    def __init__(self,baseClassifier='DT', numIt=50) -> None:
        self.baseClassifier = baseClassifier # åŸºåˆ†ç±»å™¨
        self.numIt = numIt # å¾ªç¯æ¬¡æ•°ï¼ˆåŸºåˆ†ç±»å™¨ä¸ªæ•°ï¼‰
        self.estimators = [] # å­˜å‚¨åŸºåˆ†ç±»å™¨
    
    '''
    å‡½æ•°è¯´æ˜ï¼šæ¨¡å‹è®­ç»ƒå‡½æ•°
    Parameters:
        data - è®­ç»ƒé›†ç‰¹å¾
        labels - è®­ç»ƒé›†æ ‡ç­¾
    Returns:
        è¿”å› numIt ä¸ªè®­ç»ƒåçš„åŸºåˆ†ç±»å™¨
    '''
    def fit(self,data,labels):
        m = np.shape(data)[0]
        for time in range(self.numIt):
            index = np.random.choice(m,size=(m),replace=True) # æœ‰æ”¾å›éšæœºæŠ½æ ·ï¼Œç”Ÿæˆæ ·æœ¬çš„éšæœºç´¢å¼•åºåˆ—
            datarandom = [] # å­˜å‚¨æœ‰æ”¾å›éšæœºæŠ½æ ·äº§ç”Ÿçš„æ–°è®­ç»ƒæ ·æœ¬
            for i in index:
                datarandom.append(data[i])
            datarandom = np.array(datarandom)
            # åŸºåˆ†ç±»å™¨é€‰æ‹©
            if self.baseClassifier == 'DT':
                clf = DecisionTreeClassifier()
            elif self.baseClassifier == 'SVM':
                clf = SVC()
            else:
                pass # å¯æ‰©å±•æ›´å¤šçš„åŸºåˆ†ç±»å™¨
            clf.fit(data,labels)
            self.estimators.append(clf)
        return self
    
    '''
    å‡½æ•°è¯´æ˜ï¼šé¢„æµ‹å‡½æ•°
    Parameters:
        data_test - æµ‹è¯•é›†ç‰¹å¾
    Returns:
        è¿”å›é¢„æµ‹ç»“æœ
    '''   
    def predict(self,data_test):
        m =  np.shape(data_test)[0]
        predictions = np.zeros(m)
        # èåˆæ‰€æœ‰åŸºåˆ†ç±»å™¨é¢„æµ‹ç»“æœï¼Œç­‰æƒé‡æŠ•ç¥¨äº§ç”Ÿæœ€ç»ˆçš„é¢„æµ‹ç»“æœ
        for time in range(self.numIt):
            clf = self.estimators[time]
            y_predict = clf.predict(data_test)
            predictions += np.array(y_predict)
        return [1 if i>=self.numIt/2 else 0  for i in predictions]
```
## ğŸ¥¾***Boosting***
- ä»å¤±è´¥ä¸­å­¦ä¹ 
- åŸºæœ¬æƒ³æ³•
  - ç»™æ¯ä¸ªæ ·æœ¬ä¸€ä¸ªæƒå€¼
  - $T$ è½®è¿­ä»£ï¼Œåœ¨æ¯è½®è¿­ä»£åå¢å¤§é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æƒé‡  
    <b><font color=Gold>æ›´å…³æ³¨â€œéš¾â€æ ·æœ¬</font></b>
### 1ï¸âƒ£***AdaBoost Algorithm***ğŸ§ 
- åˆå§‹ç»™æ¯ä¸ªæ ·æœ¬ç›¸ç­‰æƒé‡ä¸º $1/N$ ;
- â™»ï¸**For** $t=1,2,\ldots,T$ **Do**
  1. ç”Ÿæˆä¸€ä¸ªå‡è®¾ $C_t$ ;
  2. è®¡ç®—é”™è¯¯ç‡ $\epsilon_t$ :  
     $\epsilon_t$ = æ‰€æœ‰é”™è¯¯åˆ†ç±»æ ·æœ¬æƒé‡å’Œ    
  3. è®¡ç®— $\alpha_t$ :  
     $$\alpha_t=\frac{1}{2}\ln{\frac{1-\epsilon_t}{\epsilon_t}}$$
  4. æ›´æ–°æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼š  
     <b><font color=CornflowerBlue>æ­£ç¡®</font></b> åˆ†ç±»  
     **if** $\epsilon_t<0.5$ ğŸ”½, $\epsilon_t>0.5$ ğŸ”¼
     $$W_{new} = W_{old}*e^{-\alpha_t}$$   
       
     <b><font color=HotPink>é”™è¯¯</font></b> åˆ†ç±»  
     **if** $\epsilon_t<0.5$ ğŸ”¼, $\epsilon_t>0.5$ ğŸ”½
     $$W_{new}=W_{old}*e^{\alpha_t}$$     
       
  5. å½’ä¸€åŒ–æƒé‡ï¼ˆæƒé‡å’Œ =1ï¼‰;
- ğŸ’£èåˆæ‰€æœ‰å‡è®¾ $C_t$ , å„è‡ªæŠ•ç¥¨æƒé‡ä¸º $\alpha_t$ 
```python
class AdaBoost():
    def __init__(self,baseClassifier='DT',numIt=10) -> None:
        self.baseClassifier = baseClassifier # åŸºåˆ†ç±»å™¨
        self.numIt = numIt # å¾ªç¯æ¬¡æ•°ï¼ˆåŸºåˆ†ç±»å™¨ä¸ªæ•°ï¼‰
        self.estimators = [] # å­˜å‚¨åŸºåˆ†ç±»å™¨
        self.alphas = [] # å­˜å‚¨æŠ•ç¥¨æƒé‡
    
    '''
    å‡½æ•°è¯´æ˜ï¼šæ¨¡å‹è®­ç»ƒå‡½æ•°
    Parameters:
        data - è®­ç»ƒé›†ç‰¹å¾
        labels - è®­ç»ƒé›†æ ‡ç­¾
    Returns:
        è¿”å›è®­ç»ƒåçš„åŸºåˆ†ç±»å™¨ä»¥åŠæŠ•ç¥¨æƒé‡
    '''
    def fit(self,data,labels):
        m = np.shape(data)[0]
        W = np.ones(m) / m # æ ·æœ¬æƒé‡ï¼Œåˆå§‹ç›¸ç­‰ï¼ˆ1/æ ·æœ¬æ•°é‡ï¼‰
        aggClass = np.zeros(m)
        for i in range(self.numIt):
            # åŸºåˆ†ç±»å™¨é€‰æ‹©
            if self.baseClassifier == 'DT':
                clf = DecisionTreeClassifier()
            elif self.baseClassifier == 'SVM':
                clf = SVC()
            else:
                pass # å¯æ‰©å±•æ›´å¤šçš„åŸºåˆ†ç±»å™¨
            clf.fit(data,labels,sample_weight=W)
            baseclass = clf.predict(data)
            error = np.sum(W * np.where(baseclass != labels, 1, 0)) # è®¡ç®—åŠ æƒè¯¯å·®
            if error==0.5:
                break
            # æ ¹æ®è¯¯å·®æ›´æ–°æ ·æœ¬æƒé‡
            alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))
            if alpha==0.0: break
            self.alphas.append(alpha)
            errorArr = [1 if j else -1 for j in np.array(labels).T == baseclass]
            expon = -1 * alpha * np.array(errorArr)
            W = np.multiply(W, np.exp(expon))
            W = W / W.sum() # å½’ä¸€åŒ–
            
            self.estimators.append(clf)
            self.alphas.append(alpha)
            
            #è®¡ç®—AdaBoostè¯¯å·®ï¼Œå½“è¯¯å·®ä¸º0çš„æ—¶å€™ï¼Œé€€å‡ºå¾ªç¯
            aggClass += alpha * baseclass                                 
            errorRate =  np.sum(np.where(np.sign(aggClass) != labels, 1, 0)) / m
            if errorRate == 0.0: 
                break
        return self
    
    '''
    å‡½æ•°è¯´æ˜ï¼šé¢„æµ‹å‡½æ•°
    Parameters:
        data_test - æµ‹è¯•é›†ç‰¹å¾
    Returns:
        è¿”å›é¢„æµ‹ç»“æœ
    '''
    def predict(self,data_test):
        m = np.shape(data_test)[0]
        aggClass = np.zeros(m)
        for time in range(len(self.estimators)):
            clf = self.estimators[time]
            y_predict = clf.predict(data_test)
            aggClass += self.alphas[time] * y_predict
        return np.sign(aggClass)
```
### 2ï¸âƒ£***AdaBoostM1 Algorithm***ğŸ§ 
- åˆå§‹ç»™æ¯ä¸ªæ ·æœ¬ç›¸ç­‰æƒé‡ä¸º $1/N$ ;
- â™»ï¸**For** $t=1,2,\ldots,T$ **Do**
  1. ç”Ÿæˆä¸€ä¸ªå‡è®¾ $C_t$ ;
  2. è®¡ç®—é”™è¯¯ç‡ $\epsilon_t$ :  
     $\epsilon_t$ = æ‰€æœ‰é”™è¯¯åˆ†ç±»æ ·æœ¬æƒé‡å’Œ  
     if $\epsilon_t$ > *0.5*, åˆ™é€€å‡ºå¾ªç¯âš ï¸  
  3. è®¡ç®— $\beta_t$
     $$\beta_t=\epsilon_t/(1-\epsilon_t)$$
  4. æ›´æ–°æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼š  
     <b><font color=CornflowerBlue>æ­£ç¡®</font></b> åˆ†ç±» ğŸ”½ 
     $$W_{new}=W_{old}*\beta_t$$
     <b><font color=HotPink>é”™è¯¯</font></b> åˆ†ç±» ğŸ”¼
     $$W_{new}=W_{old}$$    
  5. å½’ä¸€åŒ–æƒé‡ï¼ˆæƒé‡å’Œ =1ï¼‰;
- ğŸ’£èåˆæ‰€æœ‰å‡è®¾ $C_t$ , å„è‡ªæŠ•ç¥¨æƒé‡ä¸º $\log{(1/\beta_t)}$
```python
class AdaBoostM1():
    def __init__(self,baseClassifier='DT',numIt=10) -> None:
        self.baseClassifier = baseClassifier # åŸºåˆ†ç±»å™¨
        self.numIt = numIt # å¾ªç¯æ¬¡æ•°ï¼ˆåŸºåˆ†ç±»å™¨ä¸ªæ•°ï¼‰
        self.estimators = [] # å­˜å‚¨åŸºåˆ†ç±»å™¨
        self.betas = [] # å­˜å‚¨æŠ•ç¥¨æƒé‡
    
    '''
    å‡½æ•°è¯´æ˜ï¼šæ¨¡å‹è®­ç»ƒå‡½æ•°
    Parameters:
        data - è®­ç»ƒé›†ç‰¹å¾
        labels - è®­ç»ƒé›†æ ‡ç­¾
    Returns:
        è¿”å›è®­ç»ƒåçš„åŸºåˆ†ç±»å™¨ä»¥åŠæŠ•ç¥¨æƒé‡
    '''
    def fit(self,data,labels):
        m = np.shape(data)[0]
        W = np.ones(m) / m
        aggClass = np.zeros(m)
        for i in range(self.numIt):
            # åŸºåˆ†ç±»å™¨é€‰æ‹©
            if self.baseClassifier == 'DT':
                clf = DecisionTreeClassifier()
            elif self.baseClassifier == 'SVM':
                clf = SVC()
            else:
                pass # å¯æ‰©å±•æ›´å¤šçš„åŸºåˆ†ç±»å™¨
            clf.fit(data,labels,sample_weight=W)
            baseclass = clf.predict(data)
            error = np.dot(W.T, baseclass != labels) # è®¡ç®—åŠ æƒè¯¯å·®
            # å¦‚æœè¯¯å·®å¤§äº0.5é€€å‡ºå¾ªç¯
            if error > 0.5:
                break
            self.estimators.append(clf)
            # æ ¹æ®è¯¯å·®æ›´æ–°æ ·æœ¬æƒé‡
            beta = float( max(error, 1e-16) / (1.0 - error))
            self.betas.append(beta)
            update = np.array([beta if j else 1 for j in np.array(labels).T == baseclass])
            W = np.multiply(W,update)
            W = W / W.sum() # å½’ä¸€åŒ–
           
            #è®¡ç®—AdaBoostM1è¯¯å·®ï¼Œå½“è¯¯å·®ä¸º0çš„æ—¶å€™ï¼Œé€€å‡ºå¾ªç¯
            aggClass += np.log(1/beta) * baseclass                                 
            errorRate =  np.sum(np.where(np.sign(aggClass) != labels, 1, 0)) / m
            if errorRate == 0.0: 
                break
        return self
    
    '''
    å‡½æ•°è¯´æ˜ï¼šé¢„æµ‹å‡½æ•°
    Parameters:
        data_test - æµ‹è¯•é›†ç‰¹å¾
    Returns:
        è¿”å›é¢„æµ‹ç»“æœ
    '''
    def predict(self,data_test):
        m = np.shape(data_test)[0]
        aggClass = np.zeros(m)
        for time in range(len(self.estimators)):
            clf = self.estimators[time]
            y_predict = clf.predict(data_test)
            aggClass += np.log(1/self.betas[time]) * y_predict
        return np.sign(aggClass)
```
## ğŸ“¡æ•°æ®æ¦‚è§ˆ
#### ğŸ”°å¯¼å…¥æ•°æ®
- **è®­ç»ƒé›†**ï¼š"./data/" ğŸ“‚è·¯å¾„ä¸‹ *train.csv* ğŸ“æ–‡ä»¶  
  åŒ…å«æ€»å…± ***57039*** æ¡æ•°æ®æ ·æœ¬ğŸ’¾  
  æ¯æ¡æ ·æœ¬åŒ…å« ***7*** ä¸ªç‰¹å¾ğŸï¼š  
  - *reviewerIDï¼Œasinï¼ŒreviewTextï¼Œoverallï¼Œvotes_upï¼Œvotes_allï¼Œlabel*
- **æµ‹è¯•é›†**ï¼š"./data/" ğŸ“‚è·¯å¾„ä¸‹ *test.csv* ğŸ“æ–‡ä»¶  
  åŒ…å«æ€»å…± ***11208*** æ¡æ•°æ®æ ·æœ¬ğŸ’¾  
  æ¯æ¡æ ·æœ¬åŒ…å« ***5*** ä¸ªç‰¹å¾ğŸï¼š  
  - *Idï¼ŒreviewerIDï¼Œasinï¼ŒreviewTextï¼Œoverall*  
  
  **æµ‹è¯•é›†æ ‡ç­¾**ï¼š"./data/" ğŸ“‚è·¯å¾„ä¸‹ *groundTruth.csv* ğŸ“æ–‡ä»¶  
  - *Idï¼ŒExpected*  
[***dataDownload*****é“¾æ¥**](https://github.com/xfkcode/MachineLearning/blob/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E4%BA%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%20Amazon%20%E7%94%A8%E6%88%B7%E8%AF%84%E8%AE%BA%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B/data.zip)ğŸ‘ˆ

[ğŸ“¢]ï¼šæµ‹è¯•é›†æ ‡ç­¾åˆ†ç¦»æ–‡ä»¶å­˜å‚¨ï¼Œ*Id* ä¸æµ‹è¯•é›†ç‰¹å¾æ ·æœ¬ä¸€ä¸€å¯¹åº”ï¼Œ*Expected* å³ *label*ã€‚
```python
# è¯»å…¥csvæ–‡ä»¶ä¸ºpandasçš„DataFrame
train_df = pd.read_csv('./data/train.csv', sep='\t')
test_df = pd.read_csv('./data/test.csv',sep='\t')
testlabels_df = pd.read_csv('./data/groundTruth.csv')
df.head(3) # æ˜¾ç¤ºå‰ä¸‰æ¡æ•°æ®
df.info() # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯,å¯æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ä¸¢å¤±
df.describe() # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡æ‘˜è¦
```
## ğŸ§ªå®éªŒæ–¹æ¡ˆ + ç‰¹å¾å·¥ç¨‹
ğŸ§°***sklearn*** æ–‡æœ¬å‘é‡åŒ–å·¥å…·  
- [***sklearn.feature_extraction.text.CountVectorizer***](https://github.com/xfkcode/MachineLearning/blob/main/python%E5%B7%A5%E5%85%B7/sklearn/sklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.ipynb)  
  å°†æ–‡æœ¬æ–‡æ¡£é›†åˆè½¬æ¢ä¸ºè®¡æ•°çŸ©é˜µã€‚
- [***sklearn.feature_extraction.text.TfidfVectorizer***](https://github.com/xfkcode/MachineLearning/blob/main/python%E5%B7%A5%E5%85%B7/sklearn/sklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.ipynb)  
  å°†æ–‡æ¡£é›†åˆè½¬æ¢ä¸ºTF-IDFåŠŸèƒ½çŸ©é˜µã€‚
### ***Test-1***âš—ï¸
æ•°æ®ğŸ§«
- ä½¿ç”¨è®­ç»ƒé›†å‰ ***2000*** æ¡æ•°æ®ä½œä¸º *Test-1* å…¨éƒ¨æ•°æ®è¿›è¡Œæ•°æ®åˆ’åˆ†æµ‹è¯•  

âš ï¸ç”µè„‘è·‘ä¸åŠ¨å¤§è§„æ¨¡æ•°æ®â—â—â—

[ğŸ“¢]ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†å…±åŒæ„å»ºè¯è¡¨å®ç°æ–‡æœ¬å‘é‡åŒ–

1. ***reviewText*** è¯„è®ºæ–‡æœ¬å‘é‡åŒ–  
   - **å…¨éƒ¨æ•°æ®** æ„å»ºè¯è¡¨ï¼Œå®ç°æ–‡æœ¬å‘é‡åŒ–
2. æ•°æ®åˆ’åˆ†âœ‚ï¸   
   è®­ç»ƒé›† : æµ‹è¯•é›† ***(8:2)***

### ***Test-2***âš—ï¸
æ•°æ®ğŸ§« 
- ä½¿ç”¨è®­ç»ƒé›†å‰ ***2000*** æ¡æ•°æ®ä½œä¸º *Test-2* è®­ç»ƒé›†æ•°æ®  
- ä½¿ç”¨æµ‹è¯•é›†å‰ ***200*** æ¡æ•°æ®ä½œä¸º *Test-2* æµ‹è¯•é›†æ•°æ®  
 
âš ï¸ç”µè„‘è·‘ä¸åŠ¨å¤§è§„æ¨¡æ•°æ®â—â—â—

[ğŸ“¢]ï¼šä»…ä½¿ç”¨è®­ç»ƒé›†æ„å»ºè¯è¡¨å®ç°æ–‡æœ¬å‘é‡åŒ–

1. ***reviewText*** è¯„è®ºæ–‡æœ¬å‘é‡åŒ–  
   - **è®­ç»ƒé›†æ•°æ®** æ„å»ºè¯è¡¨ï¼Œå®ç°æ–‡æœ¬å‘é‡åŒ–
2. æ„å»ºè®­ç»ƒé›†ã€æµ‹è¯•é›†æ•°æ®

## ğŸ•¹ï¸æ¨¡å‹æ„å»º
* ***Bagging + SVM***
* ***Bagging + å†³ç­–æ ‘***
* ***AdaBoost + SVM***
* ***AdaBoost + å†³ç­–æ ‘***
* ***AdaBoost.M1 + SVM***
* ***AdaBoost.M1 + å†³ç­–æ ‘***
### ğŸ§°***sklearn*** å®ç°
- åŸºåˆ†ç±»å™¨SVMï¼š `SVC(C=200,kernel='rbf')`
- åŸºåˆ†ç±»å™¨DTï¼š  &emsp;`DecisionTreeClassifier(max_depth=3)`
  
```python
Bagging_svm = BaggingClassifier(SVC(C=200,kernel='rbf'), n_estimators = 50)
Bagging_Dt = BaggingClassifier(DecisionTreeClassifier(max_depth=3), n_estimators = 50)
adBt_svm = AdaBoostClassifier(SVC(C=200,kernel='rbf'), algorithm='SAMME', n_estimators = 10)
adBt_DT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), algorithm='SAMME', n_estimators = 10)
```
### â™Ÿï¸**è‡ªä¸»** å®ç°
- åŸºåˆ†ç±»å™¨SVMï¼š `SVC()`
- åŸºåˆ†ç±»å™¨DTï¼š  &emsp;`DecisionTreeClassifier()`

[ğŸ”—***Bagging***](##ğŸ¥¡***Bagging***)ğŸ‘ˆ 
[ğŸ”—***Boosting***](##ğŸ¥¾***Boosting***)ğŸ‘ˆ
```python
clfbagging=Bagging().fit(x_train,y_train)
clfAdaBoost=AdaBoost().fit(x_train,y_train)
clfAdaBoostM1=AdaBoostM1().fit(x_train,y_train)
```
## ğŸ’¯å¯¹æ¯” ***Accuracy*** æ¨¡å‹æ€§èƒ½
ç»Ÿè®¡å¯¹æ¯”æ¨¡å‹å‡†ç¡®ç‡  
*Test-1* ğŸ”¬
- [x] ***Sklearn-models***
- [x] ***Self-models***
```python
Sklearn-models >>>
Bagging + SVM Accuracy : 0.795
Bagging + DT Accuracy : 0.76
AdaBoost + SVM Accuracy : 0.775
AdaBoost + DT Accuracy : 0.755
Self-models >>>
Bagging + DT Accuracy : 0.695
Bagging + SVM Accuracy : 0.77
AdaBoost + DT Accuracy : 0.665
AdaBoost + SVM Accuracy : 0.77
AdaBoostM1 + DT Accuracy : 0.675
AdaBoostM1 + SVM Accuracy : 0.77
```
## ğŸ“ˆå¯¹æ¯” ***ROC/AUC*** æ¨¡å‹æ€§èƒ½
ğŸ“ç”»å‡ºäº†æ¨¡å‹ ***ROC*** æ›²çº¿ [ğŸ”—](https://github.com/xfkcode/MachineLearning/blob/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E4%BA%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%20Amazon%20%E7%94%A8%E6%88%B7%E8%AF%84%E8%AE%BA%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B/%E5%9F%BA%E4%BA%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%20Amazon%20%E7%94%A8%E6%88%B7%E8%AF%84%E8%AE%BA%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B.ipynb)  
ğŸ’»è®¡ç®—äº† ***AUC*** æŒ‡æ ‡  
*Test-1* ğŸ”¬
- ***sklearn-models***  
  ***sklearn*** æ¨¡å‹ä½¿ç”¨ `proba` æ¯” `predict` æ•ˆæœå¥½  
  ***AUC*** æŒ‡æ ‡å‡è¶…è¿‡ ***0.6***
- ***self-models***  
è‡ªä¸»å®ç°æ¨¡å‹æ•ˆæœä¸å¥½  
ğŸ‘‰**åŸå› **ï¼š
*AdaBoost/AdaBoostM1* ç®—æ³•æ¯æ¬¡è¿­ä»£ååŸºåˆ†ç±»å™¨é”™è¯¯ç‡ä¼šå‡é«˜ï¼Œ  
å¹¶ä¿æŒåœ¨ ***0.5*** å·¦å³
ä¼šä½¿å¾—æƒé‡æ›´æ–°æ— æ³•æ­£å¸¸è¿è¡Œï¼ŒåŸºåˆ†ç±»å™¨æ ·æœ¬åŠ æƒè®­ç»ƒç»“æœå‡ºç°åå·®ã€‚  

â­•ç›®å‰è¿˜æ²¡æœ‰æ‰¾å‡ºè§£å†³åŠæ³•ï¼Œåç»­ä¼šç»§ç»­æ¢ç´¢æ›´æ–°ğŸ¤¯

---
> âœï¸ [é‚¢ç¦å‡¯ (xfkcode@github)](https://github.com/xfkcode)  
> ğŸ“… *å†™äº 2022å¹´11æœˆ*